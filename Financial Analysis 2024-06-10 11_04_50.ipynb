{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14cdc565-261e-4542-916d-7a3115cd735b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#create session\n",
    "spark = SparkSession.builder.appName('Financial Analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca89373-9d25-4d16-82de-5d194b372038",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# s3 bucket credentials\n",
    "\n",
    "AWS_ACCESS_KEY_ID='****************************'\n",
    "AWS_SECRET_ACCESS_KEY='***************************'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33784ee9-ff63-4990-8c5c-3705aaf90841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc0f03f-5c2e-4643-9261-e03ca0b4dcb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "finance_schema = StructType([\n",
    "    StructField(\"Transaction ID\", StringType(), False),\n",
    "    StructField(\"Customer ID\", StringType(), False),\n",
    "    StructField(\"Transaction Amount\", DoubleType(), False),\n",
    "    StructField(\"Transaction Date\", TimestampType(), False),\n",
    "    StructField(\"Payment Method\", StringType(), False),\n",
    "    StructField(\"Product Category\", StringType(), False),\n",
    "    StructField(\"Quantity\", IntegerType(), False),\n",
    "    StructField(\"Customer Age\", IntegerType(), False),\n",
    "    StructField(\"Customer Location\", StringType(), False),\n",
    "    StructField(\"Device Used\", StringType(), False),\n",
    "    StructField(\"IP Address\", StringType(), False),\n",
    "    StructField(\"Shipping Address\", StringType(), False),\n",
    "    StructField(\"Billing Address\", StringType(), False),\n",
    "    StructField(\"Is Fraudulent\", IntegerType(), False),\n",
    "    StructField(\"Account Age Days\", IntegerType(), False),\n",
    "    StructField(\"Transaction Hour\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ce6246-dd91-4a5a-bd91-5884d0931926",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Transaction ID: string (nullable = true)\n |-- Customer ID: string (nullable = true)\n |-- Transaction Amount: double (nullable = true)\n |-- Transaction Date: timestamp (nullable = true)\n |-- Payment Method: string (nullable = true)\n |-- Product Category: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Customer Age: integer (nullable = true)\n |-- Customer Location: string (nullable = true)\n |-- Device Used: string (nullable = true)\n |-- IP Address: string (nullable = true)\n |-- Shipping Address: string (nullable = true)\n |-- Billing Address: string (nullable = true)\n |-- Is Fraudulent: integer (nullable = true)\n |-- Account Age Days: integer (nullable = true)\n |-- Transaction Hour: integer (nullable = true)\n\n+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+-------------+--------------------+---------------+-------------+----------------+----------------+\n|      Transaction ID|         Customer ID|Transaction Amount|   Transaction Date|Payment Method|Product Category|Quantity|Customer Age|Customer Location|Device Used|   IP Address|    Shipping Address|Billing Address|Is Fraudulent|Account Age Days|Transaction Hour|\n+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+-------------+--------------------+---------------+-------------+----------------+----------------+\n|c12e07a0-8a06-4c0...|8ca9f102-02a4-420...|             42.32|2024-03-24 23:42:43|        PayPal|     electronics|       1|          40|  East Jameshaven|    desktop|110.87.246.85|5399 Rachel Strav...|           null|         null|            null|            null|\n|    North Blakeburgh|           IL 78600\"|              null|               null|          null|            null|    null|        null|             null|       null|         null|                null|           null|         null|            null|            null|\n+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+-------------+--------------------+---------------+-------------+----------------+----------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "s3_bucket = \"s3://financial-pipeline/Financial_Dataset.csv\"\n",
    "financial_df = spark.read.schema(finance_schema).format('csv').option('header', 'true').option('delimiter', ',').load(s3_bucket)\n",
    "financial_df.printSchema()\n",
    "financial_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe15921-2d9a-40ec-a0b3-2e38e37bf3fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum, count, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce8b910d-309a-4a03-8635-592986d9900a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [Table(name='transactions', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
     ]
    }
   ],
   "source": [
    "# loading the data into a pyspark table\n",
    "df = financial_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# to view all pyspark table in our cluster\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023bce5e-e284-41b1-bdb3-87b7efcf8af0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# To Answer some business Questions with our pyspark table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f230240-1ebc-43c1-aeb1-12146a0e6a89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Distribution of transaction amounts across different customer age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33417a7c-d9a8-44f9-a8cd-7103901ae01d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------------------+------------------------+\n|Customer Age|Transaction_Count|Avg_Transaction_Amount|Total_Transaction_Amount|\n+------------+-----------------+----------------------+------------------------+\n|        null|            47268|    19.772996530422272|                467315.0|\n|          -2|                1|                190.76|                  190.76|\n|           0|                8|              192.6875|                  1541.5|\n|           1|                4|                 53.19|                  212.76|\n|           2|                4|    235.95250000000001|       943.8100000000001|\n|           3|                5|               181.304|                  906.52|\n|           4|                9|    145.53333333333333|                  1309.8|\n|           5|               12|    267.19083333333333|                 3206.29|\n|           6|               20|    374.79400000000004|       7495.880000000001|\n|           7|               26|     262.6403846153846|                 6828.65|\n|           8|               24|    195.36374999999998|                 4688.73|\n|           9|               39|     260.3871794871795|                 10155.1|\n|          10|               50|    248.68699999999998|      12434.349999999999|\n|          11|               62|    203.07258064516128|                 12590.5|\n|          12|               67|     216.4449253731343|                14501.81|\n|          13|              102|    319.74186274509805|                32613.67|\n|          14|              114|     210.7080701754386|                24020.72|\n|          15|              132|    239.61075757575765|       31628.62000000001|\n|          16|              177|    244.03813559322043|      43194.750000000015|\n|          17|              198|    215.00762626262627|                42571.51|\n+------------+-----------------+----------------------+------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "age_dist = spark.sql(\"\"\"\n",
    "    SELECT `transactions`.`Customer Age`, \n",
    "           COUNT(*) AS Transaction_Count, \n",
    "           AVG(`transactions`.`Transaction Amount`) AS Avg_Transaction_Amount, \n",
    "           SUM(`transactions`.`Transaction Amount`) AS Total_Transaction_Amount\n",
    "    FROM transactions\n",
    "    GROUP BY `transactions`.`Customer Age`\n",
    "    ORDER BY `transactions`.`Customer Age`\n",
    "\"\"\")\n",
    "age_dist.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62fe6dd2-d3cf-4c76-9954-1556e4a84780",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Most popular product categories among different customer demographics (age, location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f580804-7f2b-4c20-838e-3607957a45c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------------+--------------+\n|Customer Age|Customer Location|Product Category|Category_Count|\n+------------+-----------------+----------------+--------------+\n|        null|             null|            null|         47268|\n|          38|       Lewismouth|   home & garden|             2|\n|          45|    South Matthew|   home & garden|             2|\n|          25|    East Jonathan|     electronics|             2|\n|          44|       Port Sarah|     electronics|             2|\n|          45|  West Davidville|     electronics|             2|\n|          38|       West James|   home & garden|             2|\n|          29|      Lake Steven|   home & garden|             2|\n|          37|        Johnville|     electronics|             2|\n|          31|       Lake Brian|        clothing|             2|\n|          41|   Alexandramouth|   home & garden|             2|\n|          28|       Josephfort|        clothing|             2|\n|          51|     West Michael|   home & garden|             2|\n|          53|       Lake Emily|     electronics|             2|\n|          37|  Christopherport| health & beauty|             2|\n|          24|        Kevintown|   home & garden|             2|\n|          29|        New Emily|     electronics|             2|\n|          33|      Carolynland|   home & garden|             2|\n|          28|    North Michael|    toys & games|             2|\n|          29|      Stevenshire| health & beauty|             2|\n+------------+-----------------+----------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "popular_categories = spark.sql(\"\"\"\n",
    "    SELECT `transactions`.`Customer Age`, `transactions`.`Customer Location`, `transactions`.`Product Category`, \n",
    "           COUNT(*) AS Category_Count\n",
    "    FROM transactions\n",
    "    GROUP BY `transactions`.`Customer Age`, `transactions`.`Customer Location`, `transactions`.`Product Category`\n",
    "    ORDER BY Category_Count DESC\n",
    "\"\"\")\n",
    "popular_categories.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd7b43c-072d-4574-863c-0df78f759e3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Most frequently used payment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33150531-912e-445e-8bfe-948963900aa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n|Payment_Method|Usage_Count|\n+--------------+-----------+\n|    debit card|       5952|\n|   credit card|       5923|\n|        PayPal|       5899|\n| bank transfer|       5860|\n+--------------+-----------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SQL Query\n",
    "payment_methods = spark.sql(\"\"\"\n",
    "    SELECT `transactions`.`Payment Method` AS Payment_Method, \n",
    "           COUNT(*) AS Usage_Count\n",
    "    FROM transactions\n",
    "    WHERE `transactions`.`Payment Method` IS NOT NULL\n",
    "    GROUP BY `transactions`.`Payment Method`\n",
    "    ORDER BY Usage_Count DESC\n",
    "\"\"\")\n",
    "payment_methods.show(4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f00e0904-0e7f-4914-9856-206143d6b6d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Locations with the highest transaction volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6240c83-a1a2-433b-b81c-6d787fad44b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n|Customer_Location|Transaction_Count|\n+-----------------+-----------------+\n|    North Michael|               30|\n|     East Michael|               24|\n| West Christopher|               21|\n|       East David|               20|\n|     Lake Michael|               20|\n|       Jamesmouth|               19|\n|       Smithmouth|               18|\n|      New Michael|               18|\n|      North James|               17|\n|        East John|               17|\n|    South Michael|               17|\n|       Port James|               16|\n|     South Robert|               16|\n|   South Jennifer|               16|\n|     Port Michael|               15|\n|     West Michael|               15|\n|     Lake William|               15|\n|        New David|               15|\n|       West James|               15|\n|      Lake Robert|               15|\n+-----------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "\n",
    "highest_volumes = spark.sql(\"\"\"\n",
    "    SELECT `transactions`.`Customer Location` AS Customer_Location, \n",
    "           COUNT(*) AS Transaction_Count\n",
    "    FROM transactions\n",
    "    WHERE `transactions`.`Customer Location` IS NOT NULL\n",
    "    GROUP BY `transactions`.`Customer Location`\n",
    "    ORDER BY Transaction_Count DESC\n",
    "\"\"\")\n",
    "highest_volumes.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8595585c-095a-42e9-a3b3-e3a89d658b4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Average transaction amount for each product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0fce4b5-942a-4161-9a6f-198ab16d6571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+\n|Product_Category|Avg_Transaction_Amount|\n+----------------+----------------------+\n|   home & garden|    234.90742791475174|\n|        clothing|    228.69546499255145|\n|    toys & games|     228.4037441860467|\n|     electronics|    228.21673967986555|\n| health & beauty|     226.5108713337618|\n+----------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "\n",
    "avg_trans_amount = spark.sql(\"\"\"\n",
    "    SELECT `transactions`.`Product Category` AS Product_Category, \n",
    "           AVG(`transactions`.`Transaction Amount`) AS Avg_Transaction_Amount\n",
    "    FROM transactions\n",
    "    WHERE `transactions`.`Product Category` IS NOT NULL\n",
    "    GROUP BY `transactions`.`Product Category`\n",
    "    ORDER BY Avg_Transaction_Amount DESC\n",
    "\"\"\")\n",
    "\n",
    "avg_trans_amount.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81cd1b9f-4ab5-41c2-96ac-a09ee9082efd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### For some reasons I have to stop here as I didn't really achieve what I wanted to achieve. Spark dataframe added so many null values into my dataset but once it is loaded in pandas and excel, there are no null values.\n",
    "\n",
    "### I had the intentions of adding automation in my pipeline but I am restricted because I used databrick community edition as at the time of creating this project. So yeah, I hope to come back to this project later in future after I must have added more knowledge on databricks. Thanks for sticking around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf6156bc-1a70-42cb-82f8-a13709dbaba6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Financial Analysis 2024-06-10 11:04:50",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
